---
title: "*collapse* and *data.table*"
subtitle: "Harmony and High Performance"
author: "Sebastian Krantz"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true

vignette: >
  %\VignetteIndexEntry{collapse and dplyr: Harmony and High Performance}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{css, echo=FALSE}
pre {
  max-height: 500px;
  overflow-y: auto;
}

pre[class] {
  max-height: 500px;
}
```


```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(data.table)
library(microbenchmark)
library(collapse)
knitr::opts_chunk$set(error = FALSE, message = FALSE, warning = FALSE, 
                      comment = "#", tidy = FALSE, cache = TRUE, collapse = TRUE,
                      fig.width = 8, fig.height = 5, 
                      out.width = '100%')

# knitr::opts_chunk$set(
#   comment = "#",
#     error = FALSE,
#      tidy = FALSE,
#     cache = FALSE,
#  collapse = TRUE,
#  fig.width = 8, 
#  fig.height= 5,
#  out.width='100%'
# )

RUNBENCH <- identical(Sys.getenv("RUNBENCH"), "TRUE")

oldopts <- options(width = 100L)
set.seed(101)
```

*collapse* is a C/C++ based package for data transformation and statistical computing in R. It's aims are:

1. To facilitate complex data transformation, exploration and computing tasks in R.
2. To help make R code fast, flexible, parsimonious and programmer friendly. 

This vignette focuses on using *collapse* with the popular *data.table* package by Matt Dowle and Arun Srinivasan. In contrast to *dplyr* and *plm* whose methods (*grouped_df*, *pseries*, *pdata.frame*) *collapse* supports, there is no deep integration between *collapse* and *data.table*. However *collapse* functions non-destructively handle *data.table*'s and the two packages work similarly on the C/C++ side of things, while largely complementary in functionality. 


## 1. What are the Strengths of each Package?

Needless to say both *data.table* and *collapse* are high-performance packages that work well together, but it is good to also understand where each has its strengths.

* *data.table* offers an enhanced data frame based class to contain data (including list columns). For this class it provides a concise data manipulation syntax which also includes fast aggregation / slit-apply-combine computing, joins, keying, reshaping, some time-series functionality like lagging and rolling statistics, set operations on tables and a number of very useful other functions like the fast csv reader, list-transpose etc. *data.table* makes data management, and computations on data very easy and scaleable, supporting huge datasets in a very memory efficient way. *data.table* caters well to the end user by compressing an enormous amount of functionality into two square brackets `[]`. Some of the exported functions are great for programming and also support other classes, but a lot of the functionality and optimization of *data.table* happens under the hood and can only be accessed through the non-standard evaluation table `[i, j, by]` syntax. This syntax has a cost of about 1-3 milliseconds for each call. Memory efficiency and thread-parallelization make *data.table* the star performer on huge data.   

* *collapse* is class-agnostic in nature, handling vectors, matrices, data frames and various related classes. It's functionality focuses on advanced statistical computations, by proving column-wise grouped and weighted statistical functions, fast and complex data aggregation and transformations, liner model fitting, time series and panel data computations, as well as some advanced summary statistics and recursive processing of lists of data objects. It also offers some powerful utility functions like fast data manipulation, grouping, factor generation, recoding and handling outliers. *collapse* supports both *tidyverse* and base R / standard evaluation programming, non-destructively handles all major R classes and objects, and is very programmer friendly by making accessible most of it's internal C/C++ based functionality (like grouping objects) available in R. *collapse* functions are simpler, access the underlying serial C/C++ code faster and are very strongly optimized on the R side of things (with basic function execution speeds of 10-50 microseconds). This makes *colapse* ideal for advanced and high-performance statistical programming on academically sized datasets (< 10 mio obs.). 

Thus you don't need to choose between these two. I created *collapse* because I wanted more flexibility and programmability and *data.table* fell short on my statistical demands. I still manage most of my data using *data.table*. 

## 2. How to Operate Both Packages

Applying *collapse* functions to a data.table always gives a data.table back e.g. 

```{r}
library(collapse)
library(magrittr)
library(data.table)

DT <- qDT(wlddev) # collapse::qDT converts objects to data.table using a shallow copy

# Same thing, but notice that fmean give's NA's for missing countries
DT %>% gby(country) %>% gv(9:12) %>% fmean
DT[, lapply(.SD, mean, na.rm = TRUE), by = country, .SDcols = 9:12]

# This also works without magrittr pipes with the collap() function
collap(DT, ~ country, fmean, cols = 9:12)
# ... which uses the mean by default, and collapv allows indices for grouping variables
collapv(DT, 1L, cols = 9:12)

```

At this data size *collapse* outperforms *data.table* (which might reverse as data size grows):

```{r}
library(microbenchmark)

microbenchmark(collapse = DT %>% gby(country) %>% get_vars(9:12) %>% fmean,
               data.table = DT[, lapply(.SD, mean, na.rm = TRUE), by = country, .SDcols = 9:12])

```

What is critical here is never to do something like this:

```{r}
DT[, lapply(.SD, fmean), by = country, .SDcols = 9:12]
```
The reason ist that *collapse* functions are S3 generic with methods for vectors, matrices and data frames amongst others. So you incur a method-dispatch for every column and every group the function is applied to.

```{r}
fmean
methods(fmean)
```

You may now contend that `base::mean` is also S3 generic, but in this `DT[, lapply(.SD, mean, na.rm = TRUE), by = country, .SDcols = 9:12]` code *data.table* does not use `base::mean`, but `data.table:::gmean`, an internal optimized mean function which is efficiently applied over those groups. `fmean` works similar, but makes this functionality explicit. 

```{r}
str(fmean.data.frame)
```

So here we can see the `x` argument for the data, the `g` argument for grouping vectors, a weight vector `w`, different options `TRA` to transform the original data using the computed means, and some functionality regarding missing values (defaut: removed / skipped), group names (which are added as row-names to a data frame, but not to a *data.table*) etc. So programmatically we can do

```{r}
fmean(gv(DT, 9:12), DT$country)

# Or
g <- GRP(DT, "country")
add_vars(g[["groups"]], fmean(gv(DT, 9:12), g))
```
To give us the same result obtained through the high-level functions `gby / fgroup_by` or `collap`. This is however not what *data.table* is doing in `DT[, lapply(.SD, fmean), by = country, .SDcols = 9:12]`. Since `fmean` is not a function it recognizes and is able to optimize, it does something like this,

```{r}
BY(gv(DT, 9:12), g, fmean) # using collapse::BY
```
which applies `fmean` to every group in every column of the data. So if we would want to use `fmean` inside the *data.table*, we should do something like this:

```{r}
# This does not save thr grouping columns, we are simply passing a grouping vector to g
# and aggregating the subset of the data table (.SD).
DT[, fmean(.SD, country), .SDcols = 9:12]

# If we want to keep the grouping columns, we need to group .SD first.
DT[, fmean(gby(.SD, country)), .SDcols = c(1L, 9:12)]
```
Needless to say this kind of programming seems a bit arcane, so there is actually not that great of a sope to use collapse's *Fast Statistical Functions* inside *data.table*. I drive this point home with a benchmark:
```{r}
microbenchmark(collapse = DT %>% gby(country) %>% get_vars(9:12) %>% fmean,
               data.table = DT[, lapply(.SD, mean, na.rm = TRUE), by = country, .SDcols = 9:12],
               data.table_base = DT[, lapply(.SD, base::mean, na.rm = TRUE), by = country, .SDcols = 9:12],
               hybrid_bad = DT[, lapply(.SD, fmean), by = country, .SDcols = 9:12],
               hybrid_ok = DT[, fmean(gby(.SD, country)), .SDcols = c(1L, 9:12)])

```

As you can see, *data.table* has some overhead, so there is absolutely no need to do some crazy syntax manipulation as I have shown. 

There is more scope to use *collapse* transformation functions inside *data.table*. Below I give some basic examples:

```{r}

DT[, sum_ODA := sum(ODA, na.rm = TRUE), by = country]
DT[, sum_ODA := fsum(ODA, country, TRA = "replace_fill")]  # "replace_fill" overwrites missing values, "replace" keeps them
# A native collapse solution
settfm(DT, sum_ODA = fsum(ODA, country, TRA = "replace_fill"))  # "replace_fill" overwrites missing values, 
# This computes the percentage of total ODA distributed received by 
# each country both over time and within a given year
settfm(DT, perc_c_ODA = fsum(ODA, country, TRA = "%"),
           perc_y_ODA = fsum(ODA, year, TRA = "%"))

# Centering GDP
DT[, demean_PCGDP := PCGDP - mean(PCGDP, na.rm = TRUE), by = country]
DT[, demean_PCGDP := fwithin(PCGDP, country)]

# Lagging GDP
DT[order(year), lag_PCGDP := shift(PCGDP, 1L), by = country]
DT[, lag_PCGDP := flag(PCGDP, 1L, country, year)]

# Computing a growth rate
DT[order(year), growth_PCGDP := (PCGDP / shift(PCGDP, 1L) - 1) * 100, by = country]
DT[, lag_PCGDP := fgrowth(PCGDP, 1L, 1L, country, year)] # 1 lag, 1 iteration

# Several Growth rates
DT[order(year), paste0("growth_", .c(PCGDP, LIFEEX, GINI, ODA)) := (.SD / shift(.SD, 1L) - 1) * 100, 
   by = country, .SDcols = 9:12]

DT %<>% tfm(gv(., 9:12) %>% fgrowth(1L, 1L, country, year) %>% add_stub("growth_"))

av(DT) <- DT %>% gby(country) %>% slt(year, 9:12) %>% 
    fgrowth(1L, 1L, year, keep.ids = FALSE) %>% add_stub("growth_")

settfmv(DT, 9:12, G, 1L, 1L, country, year, apply = FALSE)

```
Note that for transformations like these, *collapse* will generally be faster even on very large data because they are implemented efficiently in C++, for example `flag` computes an ordered lag without sorting the entire data first. 

```{r}
microbenchmark(
  A = DT[, sum_ODA := sum(ODA, na.rm = TRUE), by = country],
  B = DT[, sum_ODA := fsum(ODA, country, TRA = "replace_fill")],
  C = settfm(DT, sum_ODA = fsum(ODA, country, TRA = "replace_fill")),
  D = DT[order(year), lag_PCGDP := shift(PCGDP, 1L), by = country],
  E = DT[, lag_PCGDP := flag(PCGDP, 1L, country, year)],
  F = settfm(DT, lag_PCGDP = flag(PCGDP, 1L, country, year))
)

```



You can row-wise convert a matrix to data.table using mrtl(mat, names = TRUE, return = "data.table"), and you can convert a nested list of stuff to data.table using unlist2d(l, DT = TRUE).

What is important is not to do something like DT[, lapply(.SD, fmean), keyby = c("a","b","c")]. This will work but execute very slowly because collapse statistical functions like fmean are S3 generic. It is important to understand that collapse is not based around applying functions to data by groups using some mechanism as in dplyr or data.table. The grouped computation is done internally in C++. data.table internally optimizes some functions like mean or sum, which collapse allows you to do explicitly and programmatically by offering an optimized (grouped) set of statistical functions.

So to harness the speed of collapse functions we need to use the internal grouping mechanism of these functions. What you can do is something like DT[, fmean(.SD, list(a, b, c)), .SDcols = setdiff(names(DT), c("a","b","c"))], this will give you a fast computation but you are of course missing the grouping columns a, b and c in the output. So best use the fgroup_by mechanism or the collap function and just apply to data.tables as to any other data.frame. For := operations it is easier to use collapse functions inside data.table. Here are some equivalent operations:



```{r, echo=FALSE}
options(oldopts)
```


## References

Timmer, M. P., de Vries, G. J., & de Vries, K. (2015). "Patterns of Structural Change in Developing Countries." . In J. Weiss, & M. Tribe (Eds.), *Routledge Handbook of Industry and Development.* (pp. 65-83). Routledge.

Cochrane, D. & Orcutt, G. H. (1949). "Application of Least Squares Regression to Relationships Containing Auto-Correlated Error Terms". *Journal of the American Statistical Association.* 44 (245): 32–61. 

Prais, S. J. & Winsten, C. B. (1954). "Trend Estimators and Serial Correlation". *Cowles Commission Discussion Paper No. 383.* Chicago.

